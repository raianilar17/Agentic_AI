{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "987ecd18",
   "metadata": {},
   "source": [
    "<img src=\"https://learn.deeplearning.ai/assets/dlai-logo.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb40170",
   "metadata": {},
   "source": [
    "# ðŸ”¬ Lab Introduction: Tool Use and Reflective Agents\n",
    "\n",
    "In this lab, you will explore how AI agents can enhance research workflows by leveraging external tools and engaging in critical self-reflection. You'll learn how to build and integrate callable toolsâ€”such as web and academic search functions, and connect them to a language model using OpenAI's tool-calling API. Then, youâ€™ll guide the agent to not only generate content but also **reflect** on its own output, improving the quality and depth of the final report. By the end of this lab, you will have implemented a mini agent capable of searching, reasoning, and publishing structured reports in HTMLâ€”laying the foundation for more advanced multi-step and autonomous AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005ede51",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this lab, you can:\n",
    "- Chain steps into a research pipeline (**search â†’ reflection â†’ formatting**).\n",
    "- Convert natural-language output into **styled HTML** suitable for sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7917e",
   "metadata": {},
   "source": [
    "## âš™ï¸ Setup\n",
    "\n",
    "This section:\n",
    "- Loads environment variables\n",
    "- Instantiates the OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b22668fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Standard library imports\n",
    "# ================================\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# ================================\n",
    "# Third-party imports\n",
    "# ================================\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Local / project imports\n",
    "# ================================\n",
    "import research_tools\n",
    "\n",
    "# ================================\n",
    "# Environment setup\n",
    "# ================================\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "350b238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4c1bef",
   "metadata": {},
   "source": [
    "## ðŸ§° Provided Tools\n",
    "\n",
    "Youâ€™ll use two research helpers exposed in `research_tools`:\n",
    "- **`arxiv_search_tool(query, max_results)`** â€“ academic papers via arXiv API.\n",
    "- **`tavily_search_tool(query, max_results, include_images)`** â€“ general web search via Tavily."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacac323",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ `arxiv_search_tool`\n",
    "\n",
    "Searches arXiv and returns a list of papers with:\n",
    "- `title`, `authors`, `published`, `summary`, `url`, and (if available) `link_pdf`.\n",
    "\n",
    "Below, we run a quick test and print the results in a readable format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22d0eded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“„ Paper 1\n",
      "  Title     : Generalized Baer and Generalized Quasi-Baer Rings of Skew Generalized\n",
      "  Power Series\n",
      "  Authors   : M. M. Hamam, R. E. Abdel-Khalek, R. M. Salem\n",
      "  Published : 2024-05-06\n",
      "  URL       : http://arxiv.org/abs/2405.03423v2\n",
      "\n",
      "ðŸ“„ Paper 2\n",
      "  Title     : On generalized topological groups\n",
      "  Authors   : Murad Hussain, Moiz Ud Din Khan, Cenap Ã–zel\n",
      "  Published : 2012-05-17\n",
      "  URL       : http://arxiv.org/abs/1205.3915v1\n",
      "\n",
      "ðŸ“„ Paper 3\n",
      "  Title     : Weighted spherical means generated by generalized translation and\n",
      "  general Euler-Poisson-Darboux equation\n",
      "  Authors   : Elina Shishkina\n",
      "  Published : 2017-03-18\n",
      "  URL       : http://arxiv.org/abs/1703.06340v1\n",
      "\n",
      "\n",
      "ðŸ§¾ Raw Results:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"title\": \"Generalized Baer and Generalized Quasi-Baer Rings of Skew Generalized\\n  Power Series\",\n",
      "    \"authors\": [\n",
      "      \"M. M. Hamam\",\n",
      "      \"R. E. Abdel-Khalek\",\n",
      "      \"R. M. Salem\"\n",
      "    ],\n",
      "    \"published\": \"2024-05-06\",\n",
      "    \"url\": \"http://arxiv.org/abs/2405.03423v2\",\n",
      "    \"summary\": \"Let $R$ be a ring with identity, $(S,\\\\leq)$ an ordered monoid, $\\\\omega:S \\\\to\\nEnd(R)$ a monoid homomorphism, and $A= R\\\\left[\\\\left[S,\\\\omega \\\\right]\\\\right]$\\nthe ring of skew generalized power series. The concepts of generalized Baer and\\ngeneralized quasi-Baer rings are generalization of Baer and quasi-Baer rings,\\nrespectively. A ring $R$ is called generalized right Baer (generalized right\\nquasi-Baer) if for any non-empty subset $S$ (right ideal $I$) of $R$, the right\\nannihilator of $S^n \\\\space{0.1cm}(I^n)$ is generated by an idempotent for some\\npositive integer $n$. Left cases may be defined analogously. A ring $R$ is\\ncalled generalized Baer (generalized quasi-Baer) if it is both generalized\\nright and left Baer (generalized right and left quasi-Baer) ring. In this\\npaper, we examine the behavior of a skew generalized power series ring over a\\ngeneralized right Baer (generalized right quasi-Baer) ring and prove that,\\nunder specific conditions, the ring $A$ is generalized right Baer (generalized\\nright quasi-Baer) if and only if $R$ is a generalized right Baer (generalized\\nright quasi-Baer) ring.\",\n",
      "    \"link_pdf\": \"http://arxiv.org/pdf/2405.03423v2\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"On generalized topological groups\",\n",
      "    \"authors\": [\n",
      "      \"Murad Hussain\",\n",
      "      \"Moiz Ud Din Khan\",\n",
      "      \"Cenap \\u007f\\u00d6zel\"\n",
      "    ],\n",
      "    \"published\": \"2012-05-17\",\n",
      "    \"url\": \"http://arxiv.org/abs/1205.3915v1\",\n",
      "    \"summary\": \"In this work, we will introduce the notion of generalized topological groups\\nusing generalized topological structure and generalized continuity defined by\\n?A. Cs?asz?ar [2]. We will discuss some basic properties of this kind of\\nstructures and connectedness properties of this structures are given. Keywords:\\nGeneralized topology; generalized continuity; generalized topological groups;\\ngeneralized connectedness.\",\n",
      "    \"link_pdf\": \"http://arxiv.org/pdf/1205.3915v1\"\n",
      "  },\n",
      "  {\n",
      "    \"title\": \"Weighted spherical means generated by generalized translation and\\n  general Euler-Poisson-Darboux equation\",\n",
      "    \"authors\": [\n",
      "      \"Elina Shishkina\"\n",
      "    ],\n",
      "    \"published\": \"2017-03-18\",\n",
      "    \"url\": \"http://arxiv.org/abs/1703.06340v1\",\n",
      "    \"summary\": \"We consider the spherical mean generated by a multidimensional generalized\\ntranslation and general Euler-Poisson-Darboux equation corresponding to this\\nmean. The Asgeirsson property of solutions of the ultrahyperbolic equation that\\nincludes singular differential Bessel operators acting by each variable is\\nprovided.\",\n",
      "    \"link_pdf\": \"http://arxiv.org/pdf/1703.06340v1\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Test the arXiv search tool\n",
    "results = research_tools.arxiv_search_tool(\"retrieval-augmented generation\", max_results=3)\n",
    "\n",
    "# Show formatted results\n",
    "for i, paper in enumerate(results, 1):\n",
    "    if \"error\" in paper:\n",
    "        print(f\"âŒ Error: {paper['error']}\")\n",
    "    else:\n",
    "        print(f\"ðŸ“„ Paper {i}\")\n",
    "        print(f\"  Title     : {paper['title']}\")\n",
    "        print(f\"  Authors   : {', '.join(paper['authors'])}\")\n",
    "        print(f\"  Published : {paper['published']}\")\n",
    "        print(f\"  URL       : {paper['url']}\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nðŸ§¾ Raw Results:\\n\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fd399",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ `tavily_search_tool`\n",
    "\n",
    "Calls the Tavily API to fetch web results. Returns a list of dicts:\n",
    "- `title`, `content`, `url` (and optional image URLs when `include_images=True`).\n",
    "\n",
    "Run the cell to inspect sample output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6e728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'What is Retrieval Augmented Generation (RAG)? | Databricks', 'content': 'Retrieval augmented generation, or RAG, is an architectural approach that can improve the efficacy of large language model (LLM) applications by leveraging custom data. This is called retrieval augmented generation (RAG), as you would retrieve the relevant data and use it as augmented context for the LLM. With RAG architecture, organizations can deploy any LLM model and augment it to return relevant results for their organization by giving it a small amount of their data without the costs and time of fine-tuning or pretraining the model. * Using MLflow AI Gateway and Llama 2 to Build Generative AI Apps (Achieve greater accuracy using retrieval augmented generation (RAG) with your own data) Contact Databricks to schedule a demo and talk to someone about your LLM and retrieval augmented generation (RAG) projects', 'url': 'https://www.databricks.com/glossary/retrieval-augmented-generation-rag'}\n",
      "{'title': 'Top Use Cases of Retrieval-Augmented Generation (RAG) in AI', 'content': \"* By integrating retrieval mechanisms with language generation, RAG systems produce more accurate and informative text outputs, significantly improving tasks like machine translation, question answering, and summarization. Retrieval augmented generation (RAG) is an artificial intelligence methodology that combines the power of neural language models with external knowledge resources to generate text that is relevant and informed. Retrieval augmented generation (RAG) operates by integrating a retrieval component into the language generation process, expanding the model's knowledge base beyond its initial training data. Retrieval augmented generation (RAG) significantly enhances the capabilities of natural language processing systems. For **question answering**, RAG employs its retrieval component to source relevant information before generating a response.\", 'url': 'https://www.glean.com/blog/retrieval-augmented-generation-use-cases'}\n",
      "{'title': 'Retrieval Augmented Generation (RAG) â€“ 5 Use Cases - TheBlue.ai', 'content': 'In our last article, we discussed the impact of **Retrieval Augmented Generation (RAG)** systems in enhancing **Large Language Models (LLM)** by integrating dynamic information retrieval with generative processes. This new article will delve deeper into the practical applications of Retrieval Augmented Generation (RAG) systems, presenting five use cases across different industries and highlighting how these systems enhance data accessibility and streamline tasks and processes in real-world scenarios. **#2: Enhancing AI Avatars with Retrieval Augmented Generation (RAG)** Retrieval Augmented Generation (RAG) significantly improves AI avatars or digital humans by enabling them to access and utilize real-time, context-specific information during interactions. By integrating a retrieval component into generative models, RAG systems can pull from a vast repository of company-specific documents, training materials, and past queries to provide real-time, contextually relevant information to new hires.', 'url': 'https://theblue.ai/blog/rag-news/'}\n",
      "{'title': 'What is Retrieval-Augmented Generation (RAG)? - Google Cloud', 'content': 'RAG (Retrieval-Augmented Generation) is an AI framework that combines the strengths of traditional information retrieval systems (such as search and databases)', 'url': 'https://cloud.google.com/use-cases/retrieval-augmented-generation'}\n",
      "{'title': '10 Real-World Examples of Retrieval Augmented Generation', 'content': 'RAG is a powerful blend of data retrieval and generative AI models that is reshaping industries. From personalizing customer support to assisting doctors in making life-saving decisions, retrieval augmented generation services are changing the way industries work and helping them make more informed decisions using data-driven insights. > * *RAG uses data retrieval combined with generative AI to provide more accurate and contextually relevant responses for various applications.* Since RAG-powered systems combine the advantages of retrieval-based and generative models, they improve customer support by combining the capabilities of different systems. Retrieval Augmented Generation (RAG) is a powerful tool that can greatly improve the efficiency and effectiveness of research and development (R&D) processes. Retrieval-Augmented Generation (RAG) enhances the power of AI by combining information retrieval...', 'url': 'https://www.signitysolutions.com/blog/real-world-examples-of-retrieval-augmented-generation'}\n"
     ]
    }
   ],
   "source": [
    "# Test the Tavily search tool\n",
    "search_results = research_tools.tavily_search_tool(\"retrieval-augmented generation applications\")\n",
    "for item in search_results:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef7f628",
   "metadata": {},
   "source": [
    "## ðŸ”— Tool Mapping\n",
    "\n",
    "We map tool names (strings) to the actual Python functions. This allows the model to call tools by name during tool-calling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ada2e9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool mapping\n",
    "tool_mapping = {\n",
    "    \"tavily_search_tool\": research_tools.tavily_search_tool,\n",
    "    \"arxiv_search_tool\": research_tools.arxiv_search_tool,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ed9e5e",
   "metadata": {},
   "source": [
    "Gotcha. Hereâ€™s a **learner-facing prompt** that doesnâ€™t mention any internal tags, just what theyâ€™ll see (placeholders like `None`/`False`/empty blocks):\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Exercise 1: Tool-Calling Research Assistant\n",
    "\n",
    "**Goal:** Implement `generate_research_report_with_tools(prompt_)` so the model can call tools, gather evidence, and produce a sourced research report.\n",
    "\n",
    "**What youâ€™ll see in the code:**\n",
    "Some arguments are set to placeholder values (e.g., `model=None`, `messages=None`, `tools=None`, `tool_choice=None`, conditions like `if False:`, fields like `\"name\": None`, and empty temp variables). **Replace those placeholders with working code** so the loop runs end-to-end.\n",
    "\n",
    "**Constraints:**\n",
    "\n",
    "* Do **not** change the surrounding structure or helper data thatâ€™s already provided.\n",
    "* Only replace the obvious placeholders and fill in the missing pieces where indicated in the function.\n",
    "* Keep the debug prints (e.g., `ðŸ› ï¸ tool(args)`) to help you trace whatâ€™s happening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60ebc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: generate_research_report_with_tools\n",
    "def generate_research_report_with_tools(prompt_: str, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a research report using OpenAI's tool-calling with arXiv and Tavily tools.\n",
    "\n",
    "    Args:\n",
    "        prompt_ (str): The user prompt.\n",
    "        model (str): OpenAI model name.\n",
    "\n",
    "    Returns:\n",
    "        str: Final assistant research report text.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a research assistant that can search the web and arXiv to write detailed, \"\n",
    "                \"accurate, and properly sourced research reports.\\n\\n\"\n",
    "                \"ðŸ” Use tools when appropriate (e.g., to find scientific papers or web content).\\n\"\n",
    "                \"ðŸ“š Cite sources whenever relevant. Do NOT omit citations for brevity.\\n\"\n",
    "                \"ðŸŒ When possible, include full URLs (arXiv links, web sources, etc.).\\n\"\n",
    "                \"âœï¸ Use an academic tone, organize output into clearly labeled sections, and include \"\n",
    "                \"inline citations or footnotes as needed.\\n\"\n",
    "                \"ðŸš« Do not include placeholder text such as '(citation needed)' or '(citations omitted)'.\"\n",
    "            )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt_}\n",
    "    ]\n",
    "\n",
    "    functions = [research_tools.arxiv_tool_def, research_tools.tavily_tool_def]\n",
    "    MAX_TURNS = 10\n",
    "    final_text = None\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define model, messages, tools, tool_choice, and temperature\n",
    "    for _ in range(MAX_TURNS):\n",
    "        response = client.chat.completions.create( # @KEEP response = client.chat.completions.create(\n",
    "            model=model, # @REPLACE model=None,\n",
    "            messages=messages, # @REPLACE messages=None,\n",
    "            tools=functions,        # @REPLACE tools=None,\n",
    "            tool_choice=\"auto\",     # @REPLACE tool_choice=None,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "        msg = response.choices[0].message\n",
    "        messages.append(msg)\n",
    "\n",
    "        # Stop when the assistant returns a final answer (no tool calls)\n",
    "\n",
    "        # Check if there are no tool calls in the message, replace with appropriate condition\n",
    "        if not msg.tool_calls:      \n",
    "            final_text = msg.content\n",
    "            print(\"âœ… Final answer:\")\n",
    "            print(final_text)\n",
    "            break\n",
    "\n",
    "        # Execute tool calls and append results\n",
    "        for call in msg.tool_calls:\n",
    "            tool_name = call.function.name\n",
    "            args = json.loads(call.function.arguments)\n",
    "            print(f\"ðŸ› ï¸ {tool_name}({args})\")\n",
    "\n",
    "            try:\n",
    "                tool_func = tool_mapping[tool_name]\n",
    "                result = tool_func(**args)\n",
    "            except Exception as e:\n",
    "                result = {\"error\": str(e)}\n",
    "\n",
    "            # Replace None with appropriate dictionary\n",
    "            messages.append({ # @KEEP messages.append({\n",
    "                \"role\": \"tool\", # @KEEP \"role\": \"tool\",\n",
    "                \"tool_call_id\": call.id,\n",
    "                \"name\": tool_name,  # @REPLACE \"name\": None,\n",
    "                \"content\": json.dumps(result)\n",
    "            })\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return final_text or \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab0898",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ§  Exercise 2: Reflection + Rewrite\n",
    "\n",
    "**Goal:** Implement `reflection_and_rewrite(text_or_messages)` so it produces:\n",
    "\n",
    "* a structured reflection (**Strengths, Limitations, Suggestions, Opportunities**), and\n",
    "* a **revised report** that incorporates those suggestions.\n",
    "\n",
    "**What youâ€™ll see in the code:**\n",
    "Placeholders like `user_prompt = None`, `model=None`, `{\"role\":\"user\",\"content\": None}`, and `temperature=None`. **Replace those placeholders with working code.** Donâ€™t change the surrounding structure.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "* **Build the `user_prompt`** string that:\n",
    "\n",
    "   * Asks for a structured reflection with the four sections.\n",
    "   * Asks for a revised version of the report that applies the suggestions.\n",
    "   * Includes the input report text (`text`) at the end.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* The top of the function already normalizes `text_or_messages` into `text`; you donâ€™t need to modify that.\n",
    "* Keep your prompt concise but explicit about the four sections and the rewrite request.\n",
    "* Make sure you pass the actual `model` and `temperature` parameters (not hard-coded values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4585bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: reflection_and_rewrite\n",
    "def reflection_and_rewrite(text_or_messages, model: str = \"gpt-4o-mini\", temperature: float = 0.3) -> dict:\n",
    "    \"\"\"\n",
    "    Generates a structured reflection AND a revised research report.\n",
    "    Accepts raw text OR the messages list returned by generate_research_report_with_tools.\n",
    "\n",
    "    Returns:\n",
    "        dict with keys:\n",
    "          - \"reflection\": structured reflection text\n",
    "          - \"revised_report\": improved version of the input report\n",
    "    \"\"\"\n",
    "    # Extract assistant content if messages were passed\n",
    "    if isinstance(text_or_messages, list):\n",
    "        text = None\n",
    "        for m in reversed(text_or_messages):\n",
    "            role = m.get(\"role\") if isinstance(m, dict) else getattr(m, \"role\", None)\n",
    "            content = m.get(\"content\") if isinstance(m, dict) else getattr(m, \"content\", None)\n",
    "            if role == \"assistant\" and content:\n",
    "                text = content\n",
    "                break\n",
    "        if not text:\n",
    "            raise ValueError(\"No assistant text found in messages.\")\n",
    "    else:\n",
    "        text = str(text_or_messages)\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Build the user prompt\n",
    "    user_prompt = (\n",
    "        \"First, provide a structured reflection (Strengths, Limitations, Suggestions, Opportunities) \"\n",
    "        \"on the following report.\\n\\n\"\n",
    "        \"Then, write a revised version of the report that incorporates your suggestions, \"\n",
    "        \"improves clarity, and strengthens academic tone.\\n\\n\"\n",
    "        f\"Report:\\n{text}\"\n",
    "    )  # @REPLACE user_prompt = None\n",
    "\n",
    "    # Call OpenAI\n",
    "    resp = client.chat.completions.create( # @KEEP resp = client.chat.completions.create(\n",
    "        # Replace None with appropriate model variable\n",
    "        model=model, # @REPLACE model=None,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an academic reviewer and editor.\"},\n",
    "            # Add user prompt\n",
    "            {\"role\": \"user\", \"content\": user_prompt}, # @REPLACE {\"role\": \"user\", \"content\": None},\n",
    "        ],\n",
    "        # Replace None with appropriate temperature variable\n",
    "        temperature=temperature, # @REPLACE temperature=None\n",
    "    )\n",
    "\n",
    "    # Extract output\n",
    "    full_output = resp.choices[0].message.content.strip()  \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return {\n",
    "        \"reflection\": full_output,\n",
    "        \"revised_report\": full_output\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa416aa5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ðŸ§  Exercise 3: Publish the Report as HTML\n",
    "\n",
    "**Goal:** Complete `convert_report_to_html(text_or_messages, model=\"gpt-4o\")` so it returns a **valid HTML string** generated by the model.\n",
    "\n",
    "**What youâ€™ll see in the code:**\n",
    "Placeholders like `model=None`, `{\"role\":\"system\",\"content\": None}`, `{\"role\":\"user\",\"content\": None}`, and `temperature=None`. **Replace those placeholders with working code.** Do not change the surrounding structure.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- **Build the `user_prompt`** (already started for you) that:\n",
    "\n",
    "   * Asks the model to convert the plain-text report to **clean, structured HTML**.\n",
    "   * Requires **only HTML** in the response (no explanations).\n",
    "   * Includes the `text_report` content at the end.\n",
    "\n",
    "- **Return `html` string.**\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "* Use the existing `system_prompt` variable as your system message.\n",
    "* Keep the user prompt strict: â€œRespond ONLY with valid HTML.â€\n",
    "* The function already extracts `text_report` from `text_or_messages`â€”no need to modify that logic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7548464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: convert_report_to_html\n",
    "def convert_report_to_html(text_or_messages, model: str = \"gpt-4o\") -> str:\n",
    "    \"\"\"\n",
    "    Converts a plaintext research report into a styled HTML page using OpenAI.\n",
    "    Accepts raw text OR the messages list from the tool-calling step.\n",
    "    \"\"\"\n",
    "    # Inline extraction (no helper)\n",
    "    if isinstance(text_or_messages, list):\n",
    "        text_report = None\n",
    "        for m in reversed(text_or_messages):\n",
    "            role = m.get(\"role\") if isinstance(m, dict) else getattr(m, \"role\", None)\n",
    "            content = m.get(\"content\") if isinstance(m, dict) else getattr(m, \"content\", None)\n",
    "            if role == \"assistant\" and content:\n",
    "                text_report = content\n",
    "                break\n",
    "        if not text_report:\n",
    "            raise ValueError(\"No assistant text found in messages.\")\n",
    "    else:\n",
    "        text_report = str(text_or_messages)\n",
    "\n",
    "    if not text_report:\n",
    "        raise ValueError(\"Empty report text.\")\n",
    "\n",
    "    system_prompt = \"You convert plaintext reports into full clean HTML documents.\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "    # Build the user prompt instructing the model to return ONLY valid HTML\n",
    "    user_prompt = ( # @KEEP user_prompt = (\n",
    "        \"You are an expert technical writing assistant. \"\n",
    "        \"Convert the following plaintext research report into a clean, structured HTML document. \"\n",
    "        \"Include section headers, well-formatted paragraphs, and clickable links. \"\n",
    "        \"Ensure citation style is preserved.\\n\\n\"\n",
    "        \"Respond ONLY with valid HTML (no explanation).\\n\\n\"\n",
    "        f\"Report:\\n{text_report}\"\n",
    "    ) \n",
    "\n",
    "    # Call the OpenAI API with a system + user message\n",
    "    resp = client.chat.completions.create( # @KEEP resp = client.chat.completions.create(\n",
    "        model=model,  # @REPLACE model=None,\n",
    "        messages=[\n",
    "            # Add system and content here\n",
    "            {\"role\": \"system\", \"content\": system_prompt},      # @REPLACE {\"role\": \"system\", \"content\": None},\n",
    "            # Add user prompt\n",
    "            {\"role\": \"user\", \"content\": user_prompt},          # @REPLACE {\"role\": \"user\", \"content\": None}\n",
    "        ],\n",
    "        # Add temperature value\n",
    "        temperature=0.5 # @REPLACE temperature=None\n",
    "    )\n",
    "\n",
    "    # Extract the HTML from the assistant message\n",
    "    html = resp.choices[0].message.content.strip()  \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f5e51",
   "metadata": {},
   "source": [
    "### ðŸš€ End-to-End Pipeline\n",
    "\n",
    "Run this cell to execute the full workflow:\n",
    "\n",
    "1. Generate a research report (tools).\n",
    "2. Reflect on the report.\n",
    "3. Convert the report to HTML.\n",
    "\n",
    "> You should see the rendered HTML below and two concise reflections in the console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f07e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Research with tools\n",
    "prompt_ = \"Radio observations of recurrent novae\"\n",
    "preliminary_report = generate_research_report_with_tools(prompt_)\n",
    "print(\"=== Research Report (preliminary) ===\\n\")\n",
    "print(preliminary_report)\n",
    "\n",
    "# 2) Reflection on the report (use the final TEXT to avoid ambiguity)\n",
    "reflection_text = reflection_and_rewrite(preliminary_report)   # <-- pass text, not messages\n",
    "print(\"=== Reflection on Report ===\\n\")\n",
    "print(reflection_text['reflection'], \"\\n\")\n",
    "print(\"=== Revised Report ===\\n\")\n",
    "print(reflection_text['revised_report'], \"\\n\")\n",
    "\n",
    "\n",
    "# 3) Convert the report to HTML (use the TEXT and correct function name)\n",
    "html = convert_report_to_html(reflection_text['revised_report'])\n",
    "\n",
    "print(\"=== Generated HTML (preview) ===\\n\")\n",
    "print((html or \"\")[:600], \"\\n... [truncated]\\n\")\n",
    "\n",
    "# 4) Display full HTML\n",
    "display(HTML(html))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed81e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "\n",
    "# Run unit tests on your graded functions\n",
    "\n",
    "# Test 1\n",
    "unittests.test_generate_research_report_with_tools(generate_research_report_with_tools)\n",
    "\n",
    "# Test 2\n",
    "unittests.test_reflection_and_rewrite(reflection_and_rewrite)\n",
    "\n",
    "# Test 3\n",
    "unittests.test_convert_report_to_html(convert_report_to_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00231efc",
   "metadata": {},
   "source": [
    "### ðŸ“Œ â€œExpected Outputâ€ note (for the notebook text cell)\n",
    "\n",
    "> **Expected Output:**\n",
    "> These tests print pass/fail feedback for each check (type, basic behavior, and minimal content).\n",
    ">\n",
    "> * `generate_research_report_with_tools` should return a **non-trivial string** (> 50 chars).\n",
    "> * `reflection_and_rewrite` should return a **dict** with **'reflection'** and **'revised\\_report'** (both strings). The reflection should **mention** the four sections (Strengths, Limitations, Suggestions, Opportunities).\n",
    "> * `convert_report_to_html` should return a **string that looks like HTML** (e.g., includes `<html>`, `<h1>`, `<p>`, or closing tags).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afaad9ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Wrap-Up\n",
    "\n",
    "You built a mini research agent that can:\n",
    "- ðŸ”Ž call tools (arXiv + Tavily),\n",
    "- ðŸ§  reflect on its own output,\n",
    "- ðŸ“° publish a clean HTML report.\n",
    "\n",
    "Great job!\n",
    "\n",
    "### What to Submit\n",
    "- Your notebook with Exercise 1â€“3 completed.\n",
    "\n",
    "### Troubleshooting (quick)\n",
    "- **Model/tool-call loop stalls?** Lower `MAX_TURNS` or print intermediate messages.\n",
    "- **HTML looks odd?** Re-run conversion with a fresh assistant response.\n",
    "\n",
    "**Youâ€™re doneâ€”nice work!** ðŸš€\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
